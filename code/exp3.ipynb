{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driving-rolling",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logical-directive",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle as pickle\n",
    "# import os\n",
    "# import pandas as pd\n",
    "# import torch\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from transformers import *\n",
    "# from load_data import *\n",
    "# import time\n",
    "# import numpy as np\n",
    "# import random MMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qualified-flush",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contrary-argument",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os, sys\n",
    "from importlib import import_module\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Subset\n",
    "from torch.optim import SGD, Adam, AdamW\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pickle as pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from ipywidgets import FloatProgress\n",
    "from load_data import *\n",
    "from transformers import AutoTokenizer, BertForSequenceClassification, AutoConfig, AutoModelForSequenceClassification\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm, trange\n",
    "from sklearn.metrics import classification_report\n",
    "from catalyst.data.sampler import BalanceClassSampler\n",
    "import wandb\n",
    "\n",
    "# import amp\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "def seed_everything(seed):\n",
    "    \"\"\"\n",
    "    동일한 조건으로 학습을 할 때, 동일한 결과를 얻기 위해 seed를 고정시킵니다.\n",
    "    \n",
    "    Args:\n",
    "        seed: seed 정수값\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "seed = 73\n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handled-foundation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_path = ''\n",
    "num_workers = 8\n",
    "\n",
    "train_data_path = \"/opt/ml/input/data/train/train_c.tsv\"\n",
    "# Pstage/0422/nzz_results_bert-base-multilingual-cased08-03/002_accuracy_81.33%.ckpt\n",
    "model_name = 'xlm-roberta-base'\n",
    "batch_size = 32\n",
    "num_classes = 42\n",
    "# 32/1e-5\n",
    "# 64/5e-5\n",
    "num_epochs = 30\n",
    "lr =1e-5\n",
    "lr_decay_step = 4\n",
    "\n",
    "# train_log_interval = 20\n",
    "# name = model_name\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peripheral-female",
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "import math\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "class CosineAnnealingWarmupRestarts(_LRScheduler):\n",
    "    \"\"\"\n",
    "        optimizer (Optimizer): Wrapped optimizer.\n",
    "        first_cycle_steps (int): First cycle step size.\n",
    "        cycle_mult(float): Cycle steps magnification. Default: -1.\n",
    "        max_lr(float): First cycle's max learning rate. Default: 0.1.\n",
    "        min_lr(float): Min learning rate. Default: 0.001.\n",
    "        warmup_steps(int): Linear warmup step size. Default: 0.\n",
    "        gamma(float): Decrease rate of max learning rate by cycle. Default: 1.\n",
    "        last_epoch (int): The index of last epoch. Default: -1.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 optimizer : torch.optim.Optimizer,\n",
    "                 first_cycle_steps : int,\n",
    "                 cycle_mult : float = 1.,\n",
    "                 max_lr : float = 0.1,\n",
    "                 min_lr : float = 0.001,\n",
    "                 warmup_steps : int = 0,\n",
    "                 gamma : float = 1.,\n",
    "                 last_epoch : int = -1\n",
    "        ):\n",
    "        assert warmup_steps < first_cycle_steps\n",
    "        \n",
    "        self.first_cycle_steps = first_cycle_steps # first cycle step size\n",
    "        self.cycle_mult = cycle_mult # cycle steps magnification\n",
    "        self.base_max_lr = max_lr # first max learning rate\n",
    "        self.max_lr = max_lr # max learning rate in the current cycle\n",
    "        self.min_lr = min_lr # min learning rate\n",
    "        self.warmup_steps = warmup_steps # warmup step size\n",
    "        self.gamma = gamma # decrease rate of max learning rate by cycle\n",
    "        \n",
    "        self.cur_cycle_steps = first_cycle_steps # first cycle step size\n",
    "        self.cycle = 0 # cycle count\n",
    "        self.step_in_cycle = last_epoch # step size of the current cycle\n",
    "        \n",
    "        super(CosineAnnealingWarmupRestarts, self).__init__(optimizer, last_epoch)\n",
    "        \n",
    "        # set learning rate min_lr\n",
    "        self.init_lr()\n",
    "    \n",
    "    def init_lr(self):\n",
    "        self.base_lrs = []\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = self.min_lr\n",
    "            self.base_lrs.append(self.min_lr)\n",
    "    \n",
    "    def get_lr(self):\n",
    "        if self.step_in_cycle == -1:\n",
    "            return self.base_lrs\n",
    "        elif self.step_in_cycle < self.warmup_steps:\n",
    "            return [(self.max_lr - base_lr)*self.step_in_cycle / self.warmup_steps + base_lr for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            return [base_lr + (self.max_lr - base_lr) \\\n",
    "                    * (1 + math.cos(math.pi * (self.step_in_cycle-self.warmup_steps) \\\n",
    "                                    / (self.cur_cycle_steps - self.warmup_steps))) / 2\n",
    "                    for base_lr in self.base_lrs]\n",
    "\n",
    "    def step(self, epoch=None):\n",
    "        if epoch is None:\n",
    "            epoch = self.last_epoch + 1\n",
    "            self.step_in_cycle = self.step_in_cycle + 1\n",
    "            if self.step_in_cycle >= self.cur_cycle_steps:\n",
    "                self.cycle += 1\n",
    "                self.step_in_cycle = self.step_in_cycle - self.cur_cycle_steps\n",
    "                self.cur_cycle_steps = int((self.cur_cycle_steps - self.warmup_steps) * self.cycle_mult) + self.warmup_steps\n",
    "        else:\n",
    "            if epoch >= self.first_cycle_steps:\n",
    "                if self.cycle_mult == 1.:\n",
    "                    self.step_in_cycle = epoch % self.first_cycle_steps\n",
    "                    self.cycle = epoch // self.first_cycle_steps\n",
    "                else:\n",
    "                    n = int(math.log((epoch / self.first_cycle_steps * (self.cycle_mult - 1) + 1), self.cycle_mult))\n",
    "                    self.cycle = n\n",
    "                    self.step_in_cycle = epoch - int(self.first_cycle_steps * (self.cycle_mult ** n - 1) / (self.cycle_mult - 1))\n",
    "                    self.cur_cycle_steps = self.first_cycle_steps * self.cycle_mult ** (n)\n",
    "            else:\n",
    "                self.cur_cycle_steps = self.first_cycle_steps\n",
    "                self.step_in_cycle = epoch\n",
    "                \n",
    "        self.max_lr = self.base_max_lr * (self.gamma**self.cycle)\n",
    "        self.last_epoch = math.floor(epoch)\n",
    "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "            param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifth-saturn",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, classes=42, smoothing=0.50, dim=-1):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pred = pred.log_softmax(dim=self.dim)\n",
    "        with torch.no_grad():\n",
    "            # true_dist = pred.data.clone()\n",
    "            true_dist = torch.zeros_like(pred)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understanding-preservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0라벨 없이 학습시키는 과정\n",
    "\n",
    "# loss\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "criterion = LabelSmoothingLoss()\n",
    "\n",
    "# load model tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\": [\"#\", \"@\", '₩', '^']})\n",
    "\n",
    "# load dataset\n",
    "train_dataset = load_data(train_data_path)\n",
    "\n",
    "# train_df, val_df = train_test_split(train_dataset, test_size = 0.2)\n",
    "train_dataset = train_dataset[train_dataset['label']!= 0]\n",
    "t2 = train_dataset[train_dataset.label != 40]\n",
    "t3 = train_dataset[train_dataset.label == 40]\n",
    "train_df, val_df = train_test_split(t2, test_size = 0.1, stratify = t2.label) # stratify option\n",
    "\n",
    "train_df = pd.concat([train_df,t3])\n",
    "\n",
    "train_label = train_df['label'].values\n",
    "val_label = val_df['label'].values\n",
    "\n",
    "# tokenizing dataset\n",
    "tokenized_train = tokenized_dataset(train_df, tokenizer)\n",
    "tokenized_val = tokenized_dataset(val_df, tokenizer)\n",
    "\n",
    "# make dataset for pytorch\n",
    "RE_train_dataset = RE_Dataset(tokenized_train, train_label)\n",
    "RE_val_dataset = RE_Dataset(tokenized_val, val_label)\n",
    "\n",
    "\n",
    "# native training using torch\n",
    "config = AutoConfig.from_pretrained(model_name, num_labels = num_classes)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, config= config)\n",
    "model.to(device)\n",
    "\n",
    "# upsampling option\n",
    "# sampler = BalanceClassSampler(RE_train_dataset.get_classes(), 'upsampling')\n",
    "# train_loader = DataLoader(RE_train_dataset, batch_size = batch_size, sampler = sampler)#, shuffle = True)\n",
    "\n",
    "# num_workers = 8\n",
    "# batch_size = 100\n",
    "train_loader = DataLoader(RE_train_dataset, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "val_loader = DataLoader(RE_val_dataset, batch_size=batch_size, shuffle = False)\n",
    "\n",
    "# optimizer\n",
    "optim = AdamW(model.parameters(), lr = lr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vertical-divide",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scheduler\n",
    "# scheduler = ReduceLROnPlateau(optim, factor = 0.1, patience = 10)\n",
    "# scheduler = StepLR(optim, lr_decay_step, gamma=0.5)\n",
    "scheduler = CosineAnnealingLR(optim, T_max=2, eta_min=0.000000001)\n",
    "# scheduler = CosineAnnealingWarmupRestarts(optim,\n",
    "#                                           first_cycle_steps=200,\n",
    "#                                           cycle_mult=1.0,\n",
    "#                                           max_lr=0.1,\n",
    "#                                           min_lr=0.001,\n",
    "#                                           warmup_steps=50,\n",
    "#                                           gamma=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indian-router",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(project='Relation Extraction', entity = 'hwan17', name = model_name, reinit = False)\n",
    "wandb.config.update({\n",
    "    \"model_name\" : model_name,\n",
    "    \"num_epochs\" : num_epochs,\n",
    "    \"batch_size\" : batch_size,\n",
    "    \"learning_rate\" : lr,\n",
    "    \"lr_decay_step\" : lr_decay_step,\n",
    "    \"scheduler\" : scheduler,\n",
    "    \"time\" : time.strftime('%H-%M', time.localtime(time.time())),\n",
    "    \"seed\" : seed,\n",
    "    'optimizer' : optim,\n",
    "    'loss_function' : criterion\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funny-surface",
   "metadata": {},
   "outputs": [],
   "source": [
    "now = time.strftime('%H-%M', time.localtime(time.time()))\n",
    "out_dir = os.path.join(os.getcwd(), 'results_{}{}'.format(model_name,now))\n",
    "os.makedirs(out_dir, exist_ok = True)\n",
    "\n",
    "best_val_acc = 0\n",
    "best_val_loss = np.inf\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs*2): #30\n",
    "    with tqdm(train_loader, total = len(train_loader), unit = 'batch') as train_bar:\n",
    "        train_loss, train_acc, loss_value = 0,0,0\n",
    "        model.train()\n",
    "        for step, batch in enumerate(train_bar , 1):\n",
    "                \n",
    "            inputs = {key : value.to(device) for key, value in batch.items() if key != 'labels'}\n",
    "            \n",
    "#             input_ids=batch['input_ids'].to(device)\n",
    "#             attention_mask = batch['attention_mask'].to(device)\n",
    "#             token_type_ids = batch['token_type_ids'].to(device)\n",
    "\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            loss = criterion(outputs.logits, labels)\n",
    "\n",
    "            optim.zero_grad()\n",
    "#             with amp.scale_loss(loss, optim) as scaled_loss:\n",
    "#                 scale_loss.backward()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "#             correct += (torch.argmax(outputs.logits, dim = 1) == labels).sum().item()\n",
    "#             acc = correct/ len(batch['input_ids'])\n",
    "#             loss_value += loss.item()\n",
    "            preds = torch.argmax(outputs.logits, dim = 1)\n",
    "    \n",
    "            correct = (preds == labels).sum().item()\n",
    "            acc = correct / batch_size\n",
    "#             epoch_acc = acc\n",
    "\n",
    "#             epoch_loss = loss.item()\n",
    "            \n",
    "#             div_num = batch_size * step if step < num_epochs else len(batch['input_ids'])\n",
    "#             train_acc = correct/div_num\n",
    "#             train_loss = loss_value\n",
    "            scheduler.step()\n",
    "#             cur_lr = scheduler.get_last_lr()\n",
    "    \n",
    "    \n",
    "            train_bar.set_description(f'Training Epoch [{epoch+1}/{num_epochs}]')\n",
    "            train_bar.set_postfix(loss=loss.item(), acc = acc*100, lr = lr)\n",
    "            \n",
    "            wandb.log({\n",
    "                \"train_acc\" : 100.*acc,\n",
    "                \"train_loss\" : loss.item()#,\n",
    "#                 \"learning_rate\" : cur_lr\n",
    "            })\n",
    "\n",
    "        with torch.no_grad():\n",
    "            print('Calculating validation results')\n",
    "            model.eval()\n",
    "            val_loss_items = []\n",
    "            val_acc_items = []\n",
    " \n",
    "            \n",
    "            label_list = []\n",
    "            pred_list = []\n",
    "            for val_batch in val_loader:\n",
    "                val_inputs = {key : value.to(device) for key, value in val_batch.items() if key != 'labels'}\n",
    "                \n",
    "                \n",
    "           #                 input_ids=val_batch['input_ids'].to(device)\n",
    "#                 attention_mask = val_batch['attention_mask'].to(device)\n",
    "#                 token_type_ids = val_batch['token_type_ids'].to(device)\n",
    "                labels = val_batch['labels'].to(device)\n",
    "\n",
    "                outputs = model(**val_inputs)\n",
    "                preds = torch.argmax(outputs.logits, dim = 1)\n",
    "\n",
    "                loss_item = criterion(outputs.logits, labels).item()\n",
    "                acc_item = (labels == preds).sum().item()\n",
    "                \n",
    "                pred_cpu = preds.to('cpu')\n",
    "                label_list.extend(val_batch['labels'])\n",
    "                pred_list.extend(pred_cpu)\n",
    "\n",
    "                #classification report\n",
    "#                 cr = classification_report(pred_cpu,val_batch['labels'])\n",
    "#                 print(cr)\n",
    "                wandb.log({\n",
    "                    \"val_preds\" : pred_cpu,\n",
    "                    \"val_labels\" : val_batch['labels']\n",
    "#                     \"classification_report\" : cr               \n",
    "                })\n",
    "                \n",
    "                val_loss_items.append(loss_item)\n",
    "                val_acc_items.append(acc_item)\n",
    "\n",
    "            val_loss = np.sum(val_loss_items) / len(val_loader)\n",
    "            val_acc = np.sum(val_acc_items) / len(RE_val_dataset)\n",
    "            cr2 = classification_report(label_list,pred_list)\n",
    "            wandb.log({\n",
    "                \"val_acc\" : 100.*val_acc,\n",
    "                \"val_loss\" : val_loss,\n",
    "                \"classification_report\" : cr2\n",
    "        #         \"preds\" : preds.to('cpu')               \n",
    "            })\n",
    "\n",
    "#             wandb.log({\n",
    "#                 \"val_acc\" : 100.*val_acc,\n",
    "#                 \"val_loss\" : val_loss,\n",
    "#                 \"labels\" : batch['labels'],\n",
    "#                 \"preds\" : preds.to('cpu')               \n",
    "#             })\n",
    "\n",
    "            if val_acc > best_val_acc:\n",
    "                print('New best model')\n",
    "                best_val_acc = val_acc\n",
    "                if best_val_acc > 0.8:\n",
    "                    torch.save(model.state_dict(),out_dir+f'/{epoch:03}_accuracy_{val_acc:4.2%}.ckpt')\n",
    "            print(f'val_acc : {val_acc}')\n",
    "\n",
    "#         print('labels')\n",
    "#         torch.cat(label_list).unique(return_counts = True)\n",
    "#         print('preds')\n",
    "#         torch.cat(pred_list).unique(return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turned-atlanta",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "criterion = LabelSmoothingLoss()\n",
    "\n",
    "# load model tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\": [\"#\", \"@\", '₩', '^']})\n",
    "\n",
    "# load dataset\n",
    "train_dataset = load_data(train_data_path)\n",
    "\n",
    "# train_df, val_df = train_test_split(train_dataset, test_size = 0.2)\n",
    "# train_dataset = train_dataset[train_dataset['label']!= 0]\n",
    "t2 = train_dataset[train_dataset.label != 40]\n",
    "t3 = train_dataset[train_dataset.label == 40]\n",
    "train_df, val_df = train_test_split(t2, test_size = 0.1, stratify = t2.label) # stratify option\n",
    "\n",
    "train_df = pd.concat([train_df,t3])\n",
    "\n",
    "train_label = train_df['label'].values\n",
    "val_label = val_df['label'].values\n",
    "\n",
    "# tokenizing dataset\n",
    "tokenized_train = tokenized_dataset(train_df, tokenizer)\n",
    "tokenized_val = tokenized_dataset(val_df, tokenizer)\n",
    "\n",
    "# make dataset for pytorch\n",
    "RE_train_dataset = RE_Dataset(tokenized_train, train_label)\n",
    "RE_val_dataset = RE_Dataset(tokenized_val, val_label)\n",
    "\n",
    "\n",
    "# native training using torch\n",
    "# config = AutoConfig.from_pretrained(model_name, num_labels = num_classes)\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(model_name, config= config)\n",
    "# model.to(device)\n",
    "\n",
    "# upsampling option\n",
    "# sampler = BalanceClassSampler(RE_train_dataset.get_classes(), 'upsampling')\n",
    "# train_loader = DataLoader(RE_train_dataset, batch_size = batch_size, sampler = sampler)#, shuffle = True)\n",
    "\n",
    "# num_workers = 8\n",
    "# batch_size = 100\n",
    "train_loader = DataLoader(RE_train_dataset, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "val_loader = DataLoader(RE_val_dataset, batch_size=batch_size, shuffle = False)\n",
    "\n",
    "\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_name, num_labels = num_classes)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, config= config)\n",
    "# model.to(device)\n",
    "model.load_state_dict(torch.load('/opt/ml/Pstage/0422/nzz_results_bert-base-multilingual-cased08-03/002_accuracy_81.33%.ckpt'))\n",
    "model.to(device)\n",
    "# optimizer\n",
    "optim = AdamW(model.parameters(), lr = lr)\n",
    "print()\n",
    "# scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rapid-victor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run = wandb.init(project='Relation Extraction', entity = 'hwan17', name = model_name, reinit = False)\n",
    "# wandb.config.update({\n",
    "#     \"model_name\" : model_name,\n",
    "#     \"num_epochs\" : num_epochs,\n",
    "#     \"batch_size\" : batch_size,\n",
    "#     \"learning_rate\" : lr,\n",
    "#     \"lr_decay_step\" : lr_decay_step,\n",
    "#     \"scheduler\" : scheduler,\n",
    "#     \"time\" : time.strftime('%H-%M', time.localtime(time.time())),\n",
    "#     \"seed\" : seed,\n",
    "#     'optimizer' : optim,\n",
    "#     'loss_function' : criterion\n",
    "# })\n",
    "\n",
    "# now = time.strftime('%H-%M', time.localtime(time.time()))\n",
    "# out_dir = os.path.join(os.getcwd(), 'results_{}{}'.format(model_name,now))\n",
    "# os.makedirs(out_dir, exist_ok = True)\n",
    "\n",
    "best_val_acc = 0\n",
    "best_val_loss = np.inf\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs): #15\n",
    "    with tqdm(train_loader, total = len(train_loader), unit = 'batch') as train_bar:\n",
    "        train_loss, train_acc, loss_value = 0,0,0\n",
    "        model.train()\n",
    "        for step, batch in enumerate(train_bar , 1):\n",
    "                \n",
    "            inputs = {key : value.to(device) for key, value in batch.items() if key != 'labels'}\n",
    "            \n",
    "#             input_ids=batch['input_ids'].to(device)\n",
    "#             attention_mask = batch['attention_mask'].to(device)\n",
    "#             token_type_ids = batch['token_type_ids'].to(device)\n",
    "\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            loss = criterion(outputs.logits, labels)\n",
    "\n",
    "            optim.zero_grad()\n",
    "#             with amp.scale_loss(loss, optim) as scaled_loss:\n",
    "#                 scale_loss.backward()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "#             correct += (torch.argmax(outputs.logits, dim = 1) == labels).sum().item()\n",
    "#             acc = correct/ len(batch['input_ids'])\n",
    "#             loss_value += loss.item()\n",
    "            preds = torch.argmax(outputs.logits, dim = 1)\n",
    "    \n",
    "            correct = (preds == labels).sum().item()\n",
    "            acc = correct / batch_size\n",
    "#             epoch_acc = acc\n",
    "\n",
    "#             epoch_loss = loss.item()\n",
    "            \n",
    "#             div_num = batch_size * step if step < num_epochs else len(batch['input_ids'])\n",
    "#             train_acc = correct/div_num\n",
    "#             train_loss = loss_value\n",
    "            scheduler.step()\n",
    "#             cur_lr = scheduler.get_last_lr()\n",
    "    \n",
    "    \n",
    "            train_bar.set_description(f'Training Epoch [{epoch+1}/{num_epochs}]')\n",
    "            train_bar.set_postfix(loss=loss.item(), acc = acc*100, lr = lr)\n",
    "            \n",
    "            wandb.log({\n",
    "                \"train_acc\" : 100.*acc,\n",
    "                \"train_loss\" : loss.item()#,\n",
    "#                 \"learning_rate\" : cur_lr\n",
    "            })\n",
    "\n",
    "        with torch.no_grad():\n",
    "            print('Calculating validation results')\n",
    "            model.eval()\n",
    "            val_loss_items = []\n",
    "            val_acc_items = []\n",
    " \n",
    "            \n",
    "            label_list = []\n",
    "            pred_list = []\n",
    "            for val_batch in val_loader:\n",
    "                val_inputs = {key : value.to(device) for key, value in val_batch.items() if key != 'labels'}\n",
    "                \n",
    "                \n",
    "           #                 input_ids=val_batch['input_ids'].to(device)\n",
    "#                 attention_mask = val_batch['attention_mask'].to(device)\n",
    "#                 token_type_ids = val_batch['token_type_ids'].to(device)\n",
    "                labels = val_batch['labels'].to(device)\n",
    "\n",
    "                outputs = model(**val_inputs)\n",
    "                preds = torch.argmax(outputs.logits, dim = 1)\n",
    "\n",
    "                loss_item = criterion(outputs.logits, labels).item()\n",
    "                acc_item = (labels == preds).sum().item()\n",
    "                \n",
    "                pred_cpu = preds.to('cpu')\n",
    "                label_list.extend(val_batch['labels'])\n",
    "                pred_list.extend(pred_cpu)\n",
    "\n",
    "                #classification report\n",
    "#                 cr = classification_report(pred_cpu,val_batch['labels'])\n",
    "#                 print(cr)\n",
    "                wandb.log({\n",
    "                    \"val_preds\" : pred_cpu,\n",
    "                    \"val_labels\" : val_batch['labels']\n",
    "#                     \"classification_report\" : cr               \n",
    "                })\n",
    "                \n",
    "                val_loss_items.append(loss_item)\n",
    "                val_acc_items.append(acc_item)\n",
    "\n",
    "            val_loss = np.sum(val_loss_items) / len(val_loader)\n",
    "            val_acc = np.sum(val_acc_items) / len(RE_val_dataset)\n",
    "            cr2 = classification_report(label_list,pred_list)\n",
    "            wandb.log({\n",
    "                \"val_acc\" : 100.*val_acc,\n",
    "                \"val_loss\" : val_loss,\n",
    "                \"classification_report\" : cr2\n",
    "        #         \"preds\" : preds.to('cpu')               \n",
    "            })\n",
    "\n",
    "#             wandb.log({\n",
    "#                 \"val_acc\" : 100.*val_acc,\n",
    "#                 \"val_loss\" : val_loss,\n",
    "#                 \"labels\" : batch['labels'],\n",
    "#                 \"preds\" : preds.to('cpu')               \n",
    "#             })\n",
    "\n",
    "            if val_acc > best_val_acc:\n",
    "                print('New best model')\n",
    "                best_val_acc = val_acc\n",
    "                if best_val_acc > 0.8:\n",
    "                    torch.save(model.state_dict(),out_dir+f'/{epoch:03}_accuracy_{val_acc:4.2%}.ckpt')\n",
    "            print(f'val_acc : {val_acc}')\n",
    "\n",
    "#         print('labels')\n",
    "#         torch.cat(label_list).unique(return_counts = True)\n",
    "#         print('preds')\n",
    "#         torch.cat(pred_list).unique(return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "forty-legend",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from IPython.display import Audio\n",
    "Audio(\"https://upload.wikimedia.org/wikipedia/commons/0/05/Beep-09.ogg\", autoplay=True)\n",
    "\n",
    "print(classification_report(np.array(label_list),np.array(pred_list)))\n",
    "\n",
    "wkonow\n",
    "\n",
    "# validataion test\n",
    "\n",
    "# inference\n",
    "state_dict_dir = os.path.join('/opt/ml','Pstage/0421/results_xlm-roberta-base03-47/006_accuracy_72.44%.ckpt')\n",
    "\n",
    "# test_data = load_data(\"/opt/ml/input/data/test/test.tsv\")\n",
    "# test_label = test_data['label'].values\n",
    "\n",
    "# tokenized_test = tokenized_dataset(test_data, tokenizer)\n",
    "\n",
    "# config = AutoConfig.from_pretrained(model_name, num_labels = num_classes)\n",
    "# test_model = BertForSequenceClassification.from_pretrained(model_name, config= config)\n",
    "\n",
    "config2 = AutoConfig.from_pretrained(model_name, num_labels = num_classes)\n",
    "test_model = BertForSequenceClassification.from_pretrained(model_name, config = config2)\n",
    "\n",
    "test_model.load_state_dict(torch.load(state_dict_dir))\n",
    "test_model.to(device)\n",
    "\n",
    "# test_dataset = RE_Dataset(tokenized_test, test_label)\n",
    "\n",
    "# test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# test_model.eval()\n",
    "# output_pred = []\n",
    "# for data in tqdm(test_loader):\n",
    "#     with torch.no_grad():\n",
    "#         inputs = {key : value.to(device) for key, value in data.items() if key != 'labels'}\n",
    "#         pred = test_model(**inputs)\n",
    "#         result = np.argmax(pred.logits.detach().cpu().numpy(), axis = -1)\n",
    "#         print(result)\n",
    "#         output_pred.extend(result)\n",
    "        \n",
    "with torch.no_grad():\n",
    "    print('Calculating validation results')\n",
    "    test_model.eval()\n",
    "    val_loss_items = []\n",
    "    val_acc_items = []\n",
    "\n",
    "\n",
    "    label_list = []\n",
    "    pred_list = []\n",
    "    for val_batch in val_loader:\n",
    "        val_inputs = {key : value.to(device) for key, value in val_batch.items() if key != 'labels'}\n",
    "\n",
    "\n",
    "   #                 input_ids=val_batch['input_ids'].to(device)\n",
    "#                 attention_mask = val_batch['attention_mask'].to(device)\n",
    "#                 token_type_ids = val_batch['token_type_ids'].to(device)\n",
    "        labels = val_batch['labels'].to(device)\n",
    "\n",
    "        outputs = test_model(**val_inputs)\n",
    "        preds = torch.argmax(outputs.logits, dim = 1)\n",
    "\n",
    "        loss_item = criterion(outputs.logits, labels).item()\n",
    "        acc_item = (labels == preds).sum().item()\n",
    "        \n",
    "        pred_cpu = preds.to('cpu')\n",
    "        label_list.extend(val_batch['labels'])\n",
    "        pred_list.extend(pred_cpu)\n",
    "#         print('preds')\n",
    "#         print(preds.to('cpu'))\n",
    "        \n",
    "#         print('labels')\n",
    "#         print(val_batch['labels'])\n",
    "        #classification report\n",
    "        cr = classification_report(pred_cpu,val_batch['labels'])\n",
    "        print(cr)\n",
    "        wandb.log({\n",
    "            \"val_preds\" : pred_cpu,\n",
    "            \"val_labels\" : val_batch['labels'],\n",
    "            \"classification_report\" : cr               \n",
    "        })\n",
    "\n",
    "        \n",
    "        val_loss_items.append(loss_item)\n",
    "        val_acc_items.append(acc_item)\n",
    "\n",
    "    val_loss = np.sum(val_loss_items) / len(val_loader)\n",
    "    val_acc = np.sum(val_acc_items) / len(RE_val_dataset)\n",
    "    cr2 = classification_report(label_list,pred_list)\n",
    "    wandb.log({\n",
    "        \"val_acc\" : 100.*val_acc,\n",
    "        \"val_loss\" : val_loss,\n",
    "        \"classification_report\" : cr2\n",
    "#         \"preds\" : preds.to('cpu')               \n",
    "    })\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        print('New best model')\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(),out_dir+f'/{epoch:03}_accuracy_{val_acc:4.2%}.ckpt')\n",
    "    print(f'val_acc : {val_acc}')\n",
    "\n",
    "# print('labels')\n",
    "# print(torch.cat(label_list).unique(return_counts = True))\n",
    "# print('preds')\n",
    "# print(torch.cat(pred_list).unique(return_counts = True)        )\n",
    "# output = pd.DataFrame(np.array(output_pred).flatten(), columns=['pred'])\n",
    "# output.to_csv('./submission_{}.csv'.format(time.strftime('%H-%M', time.localtime(time.time()))), index = False)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# cf = confusion_matrix(preds.to('cpu'),val_batch['labels'])\n",
    "cr = classification_report(preds.to('cpu'),val_batch['labels'])\n",
    "print(cr)\n",
    "# cf\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# inference\n",
    "# hyunwoongko/kobart\n",
    "# bert-base-multilingual-cased\n",
    "model_name = 'bert-base-multilingual-cased'\n",
    "state_dict_dir = os.path.join('/opt/ml','Pstage/0422/results_bert-base-multilingual-cased09-35/000_accuracy_87.67%.ckpt')\n",
    "\n",
    "test_data = load_data(\"/opt/ml/input/data/test/test.tsv\")\n",
    "test_label = test_data['label'].values\n",
    "\n",
    "tokenized_test = tokenized_dataset(test_data, tokenizer)\n",
    "\n",
    "# config = AutoConfig.from_pretrained(model_name, num_labels = num_classes)\n",
    "# test_model = BertForSequenceClassification.from_pretrained(model_name, config= config)\n",
    "\n",
    "config2 = AutoConfig.from_pretrained(model_name, num_labels = num_classes)\n",
    "test_model = AutoModelForSequenceClassification.from_pretrained(model_name, config = config2)\n",
    "\n",
    "test_model.load_state_dict(torch.load(state_dict_dir))\n",
    "test_model.to(device)\n",
    "\n",
    "test_dataset = RE_Dataset(tokenized_test, test_label)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "test_model.eval()\n",
    "output_pred = []\n",
    "for data in tqdm(test_loader):\n",
    "    with torch.no_grad():\n",
    "        inputs = {key : value.to(device) for key, value in data.items() if key != 'labels'}\n",
    "        pred = test_model(**inputs)\n",
    "        result = np.argmax(pred.logits.detach().cpu().numpy(), axis = -1)\n",
    "        print(result)\n",
    "        output_pred.extend(result)\n",
    "        \n",
    "output = pd.DataFrame(np.array(output_pred).flatten(), columns=['pred'])\n",
    "output.to_csv('./submission_{}.csv'.format(time.strftime('%H-%M', time.localtime(time.time()))), index = False)\n",
    "\n",
    "np.array(output_pred).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "finished-satellite",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pickle as pickle\n",
    "# import os\n",
    "# import pandas as pd\n",
    "# import torch\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from transformers import *\n",
    "# from load_data import *\n",
    "# import time\n",
    "# import numpy as np\n",
    "# import random \n",
    "\n",
    "# !apt-get install g++ \n",
    "# !pip install pororo\n",
    "\n",
    "\n",
    "import random\n",
    "import os, sys\n",
    "from importlib import import_module\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Subset\n",
    "from torch.optim import SGD, Adam, AdamW\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pickle as pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from ipywidgets import FloatProgress\n",
    "from load_data import *\n",
    "from transformers import AutoTokenizer, BertForSequenceClassification, AutoConfig, AutoModelForSequenceClassification\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm, trange\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# import amp\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "def seed_everything(seed):\n",
    "    \"\"\"\n",
    "    동일한 조건으로 학습을 할 때, 동일한 결과를 얻기 위해 seed를 고정시킵니다.\n",
    "    \n",
    "    Args:\n",
    "        seed: seed 정수값\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "seed = 73\n",
    "seed_everything(seed)\n",
    "\n",
    "# label_path = ''\n",
    "num_workers = 8\n",
    "\n",
    "train_data_path = \"/opt/ml/input/data/train/train_c.tsv\"\n",
    "\n",
    "model_name = 'xlm-roberta-large'\n",
    "batch_size = 32\n",
    "num_classes = 42\n",
    "# 32/1e-5\n",
    "# 64/5e-5\n",
    "num_epochs = 10\n",
    "lr =5e-6\n",
    "lr_decay_step = 4\n",
    "\n",
    "# train_log_interval = 20\n",
    "# name = model_name\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "\n",
    "from catalyst.data.sampler import BalanceClassSampler\n",
    "\n",
    "# class RE_Dataset(torch.utils.data.Dataset):\n",
    "#   def __init__(self, tokenized_dataset, labels):\n",
    "#     self.tokenized_dataset = tokenized_dataset\n",
    "#     self.labels = labels\n",
    "\n",
    "#   def __getitem__(self, idx):\n",
    "#     item = {key: torch.tensor(val[idx]) for key, val in self.tokenized_dataset.items()}\n",
    "#     item['labels'] = torch.tensor(self.labels[idx])\n",
    "#     return item\n",
    "\n",
    "#   def __len__(self):\n",
    "#     return len(self.labels)\n",
    "\n",
    "#   def get_classes(self):\n",
    "#     return self.labels\n",
    "\n",
    "# def train():\n",
    "#   ...\n",
    "#   RE_train_dataset = RE_Dataset(tokenized_train, train_label)\n",
    "#   # BalanceClassSampler를 정의합니다. 여기선 upsampling 옵션을 주었습니다.\n",
    "#   sampler = BalanceClassSampler(RE_train_dataset.get_classes(), 'upsampling')\n",
    "#   RE_train_loader = DataLoader(RE_train_dataset, batch_size=16, sampler=sampler)\n",
    "  \n",
    "#   # 한 epoch에 모델에 들어가는 label의 분포를 살펴봅시다.\n",
    "#   label_list = []\n",
    "#   for batch in RE_train_loader:\n",
    "#       label_list.append(batch['labels'])\n",
    "#   torch.cat(label_list).unique(return_counts=True)\n",
    "\n",
    "\n",
    "\n",
    "# # Dataset 구성.\n",
    "# class RE_Dataset(torch.utils.data.Dataset):\n",
    "#   def __init__(self, tokenized_dataset, labels):\n",
    "#     self.tokenized_dataset = tokenized_dataset\n",
    "#     self.labels = labels\n",
    "    \n",
    "#   def __getitem__(self, idx):\n",
    "#     item = {key: torch.tensor(val[idx]) for key, val in self.tokenized_dataset.items()}\n",
    "#     item['labels'] = torch.tensor(self.labels[idx])\n",
    "#     return item\n",
    "\n",
    "#   def __len__(self):\n",
    "#     return len(self.labels)\n",
    "\n",
    "#   def get_classes(self):\n",
    "#     return self.labels\n",
    "\n",
    "# # 처음 불러온 tsv 파일을 원하는 형태의 DataFrame으로 변경 시켜줍니다.\n",
    "# # 변경한 DataFrame 형태는 baseline code description 이미지를 참고해주세요.\n",
    "# def preprocessing_dataset(dataset, label_type):\n",
    "#   label = []\n",
    "#   for i in dataset[8]:\n",
    "#     if i == 'blind':\n",
    "#       label.append(100)\n",
    "#     else:\n",
    "#       label.append(label_type[i])\n",
    "#   out_dataset = pd.DataFrame({'sentence':dataset[1],'entity_01':dataset[2],'entity_02':dataset[5],'label':label,})\n",
    "#   return out_dataset\n",
    "\n",
    "# # tsv 파일을 불러옵니다.\n",
    "# def load_data(dataset_dir):\n",
    "#   # load label_type, classes\n",
    "#   with open('/opt/ml/input/data/label_type.pkl', 'rb') as f:\n",
    "#     label_type = pickle.load(f)\n",
    "#   # load dataset\n",
    "#   dataset = pd.read_csv(dataset_dir, delimiter='\\t', header=None)\n",
    "#   # preprecessing dataset\n",
    "#   dataset = preprocessing_dataset(dataset, label_type)\n",
    "  \n",
    "#   return dataset\n",
    "\n",
    "# # bert input을 위한 tokenizing.\n",
    "# # tip! 다양한 종류의 tokenizer와 special token들을 활용하는 것으로도 새로운 시도를 해볼 수 있습니다.\n",
    "# # baseline code에서는 2가지 부분을 활용했습니다.\n",
    "# def tokenized_dataset(dataset, tokenizer):\n",
    "#   concat_entity = []\n",
    "#   for e01, e02 in zip(dataset['entity_01'], dataset['entity_02']):\n",
    "#     temp = ''\n",
    "#     temp = e01 + '[SEP]' + e02\n",
    "#     concat_entity.append(temp)\n",
    "#   tokenized_sentences = tokenizer(\n",
    "#       concat_entity,\n",
    "#       list(dataset['sentence']),\n",
    "#       return_tensors=\"pt\",\n",
    "#       padding=True,\n",
    "#       truncation=True,\n",
    "#       max_length=100,\n",
    "#       add_special_tokens=True,\n",
    "#       )\n",
    "#   return tokenized_sentences\n",
    "\n",
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, classes=42, smoothing=0.50, dim=-1):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pred = pred.log_softmax(dim=self.dim)\n",
    "        with torch.no_grad():\n",
    "            # true_dist = pred.data.clone()\n",
    "            true_dist = torch.zeros_like(pred)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))\n",
    "    \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=0, alpha=None, size_average=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        if isinstance(alpha,(float,int,long)): self.alpha = torch.Tensor([alpha,1-alpha])\n",
    "        if isinstance(alpha,list): self.alpha = torch.Tensor(alpha)\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        if input.dim()>2:\n",
    "            input = input.view(input.size(0),input.size(1),-1)  # N,C,H,W => N,C,H*W\n",
    "            input = input.transpose(1,2)    # N,C,H*W => N,H*W,C\n",
    "            input = input.contiguous().view(-1,input.size(2))   # N,H*W,C => N*H*W,C\n",
    "        target = target.view(-1,1)\n",
    "\n",
    "        logpt = F.log_softmax(input)\n",
    "        logpt = logpt.gather(1,target)\n",
    "        logpt = logpt.view(-1)\n",
    "        pt = Variable(logpt.data.exp())\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            if self.alpha.type()!=input.data.type():\n",
    "                self.alpha = self.alpha.type_as(input.data)\n",
    "            at = self.alpha.gather(0,target.data.view(-1))\n",
    "            logpt = logpt * Variable(at)\n",
    "\n",
    "        loss = -1 * (1-pt)**self.gamma * logpt\n",
    "        if self.size_average: return loss.mean()\n",
    "        else: return loss.sum()\n",
    "\n",
    "\n",
    "\n",
    "# scheduler = StepLR(optim, factor = 0.1, patience = 10)\n",
    "\n",
    "# loss\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "criterion = LabelSmoothingLoss()\n",
    "\n",
    "# load model tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\": [\"#\", \"@\", '₩', '^']})\n",
    "\n",
    "# load dataset\n",
    "train_dataset = load_data(train_data_path)\n",
    "\n",
    "# train_df, val_df = train_test_split(train_dataset, test_size = 0.2)\n",
    "\n",
    "t2 = train_dataset[train_dataset.label != 40]\n",
    "t3 = train_dataset[train_dataset.label == 40]\n",
    "train_df, val_df = train_test_split(t2, test_size = 0.2, stratify = t2.label) # stratify option\n",
    "\n",
    "train_df = pd.concat([train_df,t3])\n",
    "\n",
    "train_label = train_df['label'].values\n",
    "val_label = val_df['label'].values\n",
    "\n",
    "# tokenizing dataset\n",
    "tokenized_train = tokenized_dataset(train_df, tokenizer)\n",
    "tokenized_val = tokenized_dataset(val_df, tokenizer)\n",
    "\n",
    "# make dataset for pytorch\n",
    "RE_train_dataset = RE_Dataset(tokenized_train, train_label)\n",
    "RE_val_dataset = RE_Dataset(tokenized_val, val_label)\n",
    "\n",
    "\n",
    "# native training using torch\n",
    "config = AutoConfig.from_pretrained(model_name, num_labels = num_classes)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, config= config)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.to(device)\n",
    "\n",
    "# upsampling option\n",
    "# sampler = BalanceClassSampler(RE_train_dataset.get_classes(), 'upsampling')\n",
    "# train_loader = DataLoader(RE_train_dataset, batch_size = batch_size, num_workers = num_workers, sampler = sampler)#, shuffle = True)\n",
    "\n",
    "# num_workers = 8\n",
    "# batch_size = 100\n",
    "train_loader = DataLoader(RE_train_dataset, batch_size = batch_size, num_workers = num_workers, shuffle = True)\n",
    "\n",
    "val_loader = DataLoader(RE_val_dataset, batch_size=batch_size, num_workers = num_workers,shuffle = False)\n",
    "\n",
    "# optimizer\n",
    "optim = AdamW(model.parameters(), lr = lr)\n",
    "\n",
    "# scheduler\n",
    "# scheduler = ReduceLROnPlateau(optim, factor = 0.1, patience = 10)\n",
    "# scheduler = StepLR(optim, lr_decay_step, gamma=0.5)\n",
    "scheduler = CosineAnnealingLR(optim, T_max=2, eta_min=0.0000000001)\n",
    "\n",
    "import wandb\n",
    "\n",
    "run = wandb.init(project='Relation Extraction', entity = 'hwan17', name = model_name, reinit = False)\n",
    "wandb.config.update({\n",
    "    \"model_name\" : model_name,\n",
    "    \"num_epochs\" : num_epochs,\n",
    "    \"batch_size\" : batch_size,\n",
    "    \"learning_rate\" : lr,\n",
    "    \"lr_decay_step\" : lr_decay_step,\n",
    "    \"scheduler\" : scheduler,\n",
    "    \"time\" : time.strftime('%H-%M', time.localtime(time.time())),\n",
    "    \"seed\" : seed,\n",
    "    'optimizer' : optim,\n",
    "    'loss_function' : criterion\n",
    "})\n",
    "\n",
    "now = time.strftime('%H-%M', time.localtime(time.time()))\n",
    "out_dir = os.path.join(os.getcwd(), 'results_{}{}'.format(model_name,now))\n",
    "os.makedirs(out_dir, exist_ok = True)\n",
    "\n",
    "best_val_acc = 0\n",
    "best_val_loss = np.inf\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    with tqdm(train_loader, total = len(train_loader), unit = 'batch') as train_bar:\n",
    "        train_loss, train_acc, loss_value = 0,0,0\n",
    "        model.train()\n",
    "        for step, batch in enumerate(train_bar , 1):\n",
    "                \n",
    "            inputs = {key : value.to(device) for key, value in batch.items() if key != 'labels'}\n",
    "            \n",
    "#             input_ids=batch['input_ids'].to(device)\n",
    "#             attention_mask = batch['attention_mask'].to(device)\n",
    "#             token_type_ids = batch['token_type_ids'].to(device)\n",
    "\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            loss = criterion(outputs.logits, labels)\n",
    "\n",
    "            optim.zero_grad()\n",
    "#             with amp.scale_loss(loss, optim) as scaled_loss:\n",
    "#                 scale_loss.backward()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "#             correct += (torch.argmax(outputs.logits, dim = 1) == labels).sum().item()\n",
    "#             acc = correct/ len(batch['input_ids'])\n",
    "#             loss_value += loss.item()\n",
    "            preds = torch.argmax(outputs.logits, dim = 1)\n",
    "    \n",
    "            correct = (preds == labels).sum().item()\n",
    "            acc = correct / batch_size\n",
    "#             epoch_acc = acc\n",
    "\n",
    "#             epoch_loss = loss.item()\n",
    "            \n",
    "#             div_num = batch_size * step if step < num_epochs else len(batch['input_ids'])\n",
    "#             train_acc = correct/div_num\n",
    "#             train_loss = loss_value\n",
    "            scheduler.step()\n",
    "            cur_lr = scheduler.get_last_lr()\n",
    "    \n",
    "    \n",
    "            train_bar.set_description(f'Training Epoch [{epoch+1}/{num_epochs}]')\n",
    "            train_bar.set_postfix(loss=loss.item(), acc = acc*100, lr = cur_lr)\n",
    "            \n",
    "            wandb.log({\n",
    "                \"train_acc\" : 100.*acc,\n",
    "                \"train_loss\" : loss.item(),\n",
    "                \"learning_rate\" : cur_lr\n",
    "            })\n",
    "\n",
    "        with torch.no_grad():\n",
    "            print('Calculating validation results')\n",
    "            model.eval()\n",
    "            val_loss_items = []\n",
    "            val_acc_items = []\n",
    "\n",
    "            \n",
    "            label_list = []\n",
    "            pred_list = []\n",
    "            for val_batch in val_loader:\n",
    "                val_inputs = {key : value.to(device) for key, value in val_batch.items() if key != 'labels'}\n",
    "                \n",
    "                \n",
    "           #                 input_ids=val_batch['input_ids'].to(device)\n",
    "#                 attention_mask = val_batch['attention_mask'].to(device)\n",
    "#                 token_type_ids = val_batch['token_type_ids'].to(device)\n",
    "                labels = val_batch['labels'].to(device)\n",
    "\n",
    "                outputs = model(**val_inputs)\n",
    "                preds = torch.argmax(outputs.logits, dim = 1)\n",
    "\n",
    "                loss_item = criterion(outputs.logits, labels).item()\n",
    "                acc_item = (labels == preds).sum().item()\n",
    "                \n",
    "                pred_cpu = preds.to('cpu')\n",
    "                label_list.extend(val_batch['labels'])\n",
    "                pred_list.extend(pred_cpu)\n",
    "\n",
    "                #classification report\n",
    "#                 cr = classification_report(pred_cpu,val_batch['labels'])\n",
    "#                 print(cr)\n",
    "                wandb.log({\n",
    "                    \"val_preds\" : pred_cpu,\n",
    "                    \"val_labels\" : val_batch['labels']\n",
    "#                     \"classification_report\" : cr               \n",
    "                })\n",
    "                \n",
    "                val_loss_items.append(loss_item)\n",
    "                val_acc_items.append(acc_item)\n",
    "\n",
    "            val_loss = np.sum(val_loss_items) / len(val_loader)\n",
    "            val_acc = np.sum(val_acc_items) / len(RE_val_dataset)\n",
    "            cr2 = classification_report(label_list,pred_list)\n",
    "#             wandb.log({\n",
    "#                 \"val_acc\" : 100.*val_acc,\n",
    "#                 \"val_loss\" : val_loss,\n",
    "#                 \"classification_report\" : cr2\n",
    "#         #         \"preds\" : preds.to('cpu')               \n",
    "#             })\n",
    "\n",
    "            wandb.log({\n",
    "                \"val_acc\" : 100.*val_acc,\n",
    "                \"val_loss\" : val_loss,\n",
    "                \"labels\" : batch['labels'],\n",
    "                \"preds\" : preds.to('cpu')               \n",
    "            })\n",
    "\n",
    "            if val_acc > best_val_acc:\n",
    "                print('New best model')\n",
    "                best_val_acc = val_acc\n",
    "                torch.save(model.state_dict(),out_dir+f'/{epoch:03}_accuracy_{val_acc:4.2%}.ckpt')\n",
    "            print(f'val_acc : {val_acc}')\n",
    "\n",
    "#         print('labels')\n",
    "#         torch.cat(label_list).unique(return_counts = True)\n",
    "#         print('preds')\n",
    "#         torch.cat(pred_list).unique(return_counts = True)\n",
    "\n",
    "print(torch.argmax(outputs.logits, dim = 1) )\n",
    "print(labels)\n",
    "\n",
    "correct = (torch.argmax(outputs.logits, dim = 1) == labels).sum().item()\n",
    "correct/len(labels)\n",
    "\n",
    "# # model(**RE_train_dataset[0])\n",
    "# input_ids = RE_train_dataset[0]['input_ids'].to(device)\n",
    "# attention_mask = RE_train_dataset[0]['attention_mask'].to(device)\n",
    "# labels = RE_train_dataset[0]['labels'].to(device)\n",
    "\n",
    "# model(input_ids, attention_mask, labels)\n",
    "\n",
    "from IPython.display import Audio\n",
    "Audio(\"https://upload.wikimedia.org/wikipedia/commons/0/05/Beep-09.ogg\", autoplay=True)\n",
    "\n",
    "print(classification_report(np.array(label_list),np.array(pred_list)))\n",
    "\n",
    "wkonow\n",
    "\n",
    "# labels = val_batch['labels'].to(device)\n",
    "\n",
    "#         outputs = test_model(**val_inputs)\n",
    "#         preds = torch.argmax(outputs.logits, dim = 1)\n",
    "\n",
    "#         loss_item = criterion(outputs.logits, labels).item()\n",
    "#         acc_item = (labels == preds).sum().item()\n",
    "        \n",
    "        \n",
    "#         pred_cpu = preds.to('cpu')\n",
    "#         label_list.extend(val_batch['labels'])\n",
    "#         pred_list.extend(pred_cpu)\n",
    "        \n",
    "pred_max = torch.max(outputs.logits, dim = 1)\n",
    "pred_maxx = np.array(pred_max.values.to('cpu'))\n",
    "predss = np.array(preds.to('cpu'))\n",
    "# labels = np.array(labels.to('cpu'))\n",
    "check = list(zip(pred_maxx, predss, labels))\n",
    "\n",
    "correct_val = [x[0] for x in check if x[1] == 0 and x[1] == x[2]]\n",
    "incorrect_val = [x[0] for x in check if x[1] == 0 and x[1] != x[2]]\n",
    "\n",
    "corrects = []\n",
    "incorrects = []\n",
    "print(correct_val)\n",
    "print(incorrect_val)\n",
    "\n",
    "# validataion test\n",
    "\n",
    "# inference\n",
    "state_dict_dir = os.path.join('/opt/ml','Pstage/0421/results_xlm-roberta-base04-12/006_accuracy_72.44%.ckpt')\n",
    "state_dict_dir2 = os.path.join('/opt/ml','Pstage/0421/results_xlm-roberta-base06-45/004_accuracy_78.99%.ckpt')\n",
    "# test_data = load_data(\"/opt/ml/input/data/test/test.tsv\")\n",
    "# test_label = test_data['label'].values\n",
    "\n",
    "# tokenized_test = tokenized_dataset(test_data, tokenizer)\n",
    "\n",
    "# config = AutoConfig.from_pretrained(model_name, num_labels = num_classes)\n",
    "# test_model = BertForSequenceClassification.from_pretrained(model_name, config= config)\n",
    "\n",
    "config2 = AutoConfig.from_pretrained(model_name, num_labels = num_classes)\n",
    "test_model = AutoModelForSequenceClassification.from_pretrained(model_name, config = config2)\n",
    "\n",
    "test_model.load_state_dict(torch.load(state_dict_dir))\n",
    "test_model.to(device)\n",
    "\n",
    "config2 = AutoConfig.from_pretrained(model_name, num_labels = num_classes)\n",
    "test_model2 = AutoModelForSequenceClassification.from_pretrained(model_name, config = config2)\n",
    "\n",
    "test_model2.load_state_dict(torch.load(state_dict_dir2))\n",
    "test_model2.to(device)\n",
    "\n",
    "# test_dataset = RE_Dataset(tokenized_test, test_label)\n",
    "\n",
    "# test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# test_model.eval()\n",
    "# output_pred = []\n",
    "# for data in tqdm(test_loader):\n",
    "#     with torch.no_grad():\n",
    "#         inputs = {key : value.to(device) for key, value in data.items() if key != 'labels'}\n",
    "#         pred = test_model(**inputs)\n",
    "#         result = np.argmax(pred.logits.detach().cpu().numpy(), axis = -1)\n",
    "#         print(result)\n",
    "#         output_pred.extend(result)\n",
    "corrects = []\n",
    "incorrects = []\n",
    "corrects2 = []\n",
    "incorrects2 = []\n",
    "with torch.no_grad():\n",
    "    print('Calculating validation results')\n",
    "    test_model.eval()\n",
    "    val_loss_items = []\n",
    "    val_acc_items = []\n",
    "\n",
    "\n",
    "    label_list = []\n",
    "    pred_list = []\n",
    "    for val_batch in val_loader:\n",
    "        val_inputs = {key : value.to(device) for key, value in val_batch.items() if key != 'labels'}\n",
    "\n",
    "\n",
    "   #                 input_ids=val_batch['input_ids'].to(device)\n",
    "#                 attention_mask = val_batch['attention_mask'].to(device)\n",
    "#                 token_type_ids = val_batch['token_type_ids'].to(device)\n",
    "        labels = val_batch['labels'].to(device)\n",
    "\n",
    "        outputs = test_model(**val_inputs)\n",
    "        preds = torch.argmax(outputs.logits, dim = 1)\n",
    "\n",
    "        loss_item = criterion(outputs.logits, labels).item()\n",
    "        acc_item = (labels == preds).sum().item()\n",
    "        \n",
    "        \n",
    "        pred_cpu = preds.to('cpu')\n",
    "        label_list.extend(val_batch['labels'])\n",
    "        pred_list.extend(pred_cpu)\n",
    "#         print('preds')\n",
    "#         print(preds.to('cpu'))\n",
    "        \n",
    "#         print('labels')\n",
    "#         print(val_batch['labels'])\n",
    "        #classification report\n",
    "#         cr = classification_report(pred_cpu,val_batch['labels'])\n",
    "#         print(cr)\n",
    "        wandb.log({\n",
    "            \"val_preds\" : pred_cpu,\n",
    "            \"val_labels\" : val_batch['labels'],\n",
    "            \"classification_report\" : cr               \n",
    "        })\n",
    "\n",
    "        \n",
    "#         val_loss_items.append(loss_item)\n",
    "#         val_acc_items.append(acc_item)\n",
    "        \n",
    "        pred_max = torch.max(outputs.logits, dim = 1)\n",
    "        pred_maxx = np.array(pred_max.values.to('cpu'))\n",
    "        predss = np.array(preds.to('cpu'))\n",
    "        # labels = np.array(labels.to('cpu'))\n",
    "        check = list(zip(pred_maxx, predss, labels))\n",
    "\n",
    "        correct_val = [x[0] for x in check if x[1] == 0 and x[1] == x[2]]\n",
    "        incorrect_val = [x[0] for x in check if x[1] == 0 and x[1] != x[2]]\n",
    "#         for x in check:\n",
    "        \n",
    "        corrects.extend(correct_val)\n",
    "        incorrects.extend(incorrect_val)\n",
    "        #####################\n",
    "        \n",
    "        outputs2 = test_model2(**val_inputs)\n",
    "        preds2 = torch.argmax(outputs2.logits, dim = 1)\n",
    "\n",
    "        loss_item2 = criterion(outputs2.logits, labels).item()\n",
    "        acc_item2 = (labels == preds2).sum().item()\n",
    "        \n",
    "        \n",
    "        pred_max2 = torch.max(outputs2.logits, dim = 1)\n",
    "        pred_maxx2 = np.array(pred_max2.values.to('cpu'))\n",
    "        predss2 = np.array(preds2.to('cpu'))\n",
    "        # labels = np.array(labels.to('cpu'))\n",
    "        check2 = list(zip(pred_maxx2, predss2, labels))\n",
    "\n",
    "        correct_val2 = [x[0] for x in check2 if x[1] != 0 and x[1] == x[2]]\n",
    "        incorrect_val2 = [x[0] for x in check2 if x[1] != 0 and x[1] != x[2]]\n",
    "#         for x in check:\n",
    "        \n",
    "        corrects2.extend(correct_val2)\n",
    "        incorrects2.extend(incorrect_val2)\n",
    "        out1 = list(map(lambda x : (np.argmax(x), np.max(x)), np.array(outputs.logits.to('cpu'))))\n",
    "        out2 = list(map(lambda x : (np.argmax(x), np.max(x)), np.array(outputs2.logits.to('cpu'))))\n",
    "        print(labels)\n",
    "        print(out1)\n",
    "        print(out2)\n",
    "        \n",
    "        \n",
    "        outputs3 = outputs.logits + outputs2.logits\n",
    "        preds = torch.argmax(outputs3, dim = 1)\n",
    "\n",
    "        loss_item = criterion(outputs3, labels).item()\n",
    "        acc_item = (labels == preds).sum().item()\n",
    "        \n",
    "        val_loss_items.append(loss_item)\n",
    "        val_acc_items.append(acc_item)\n",
    "\n",
    "    \n",
    "    val_loss = np.sum(val_loss_items) / len(val_loader)\n",
    "    val_acc = np.sum(val_acc_items) / len(RE_val_dataset)\n",
    "    cr2 = classification_report(label_list,pred_list)\n",
    "    wandb.log({\n",
    "        \"val_acc\" : 100.*val_acc,\n",
    "        \"val_loss\" : val_loss,\n",
    "        \"classification_report\" : cr2\n",
    "#         \"preds\" : preds.to('cpu')               \n",
    "    })\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        print('New best model')\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(),out_dir+f'/{epoch:03}_accuracy_{val_acc:4.2%}.ckpt')\n",
    "    print(f'val_acc : {val_acc}')\n",
    "\n",
    "# print('labels')\n",
    "# print(torch.cat(label_list).unique(return_counts = True))\n",
    "# print('preds')\n",
    "# print(torch.cat(pred_list).unique(return_counts = True)        )\n",
    "# output = pd.DataFrame(np.array(output_pred).flatten(), columns=['pred'])\n",
    "# output.to_csv('./submission_{}.csv'.format(time.strftime('%H-%M', time.localtime(time.time()))), index = False)\n",
    "\n",
    "print(len(corrects))\n",
    "print(len(incorrects))\n",
    "print(np.average(corrects)) # 0이라고 예측했을 때 맞은 값\n",
    "print(np.average(incorrects)) # 0이라고 예측했을 때 틀린 값\n",
    "print(len(corrects2))\n",
    "print(len(incorrects2))\n",
    "print(np.average(corrects2)) # 0 이 아닌 값을 맞게 예측한 값\n",
    "print(np.average(incorrects2)) # 0 이 아닌 값을 틀리게 예측한 값\n",
    "\n",
    "\n",
    "incorrects\n",
    "\n",
    "train_dataset.label.value_counts()\n",
    "\n",
    "out1 = list(map(lambda x : (np.argmax(x), np.max(x)), np.array(outputs.logits.to('cpu'))))\n",
    "out2 = list(map(lambda x : (np.argmax(x), np.max(x)), np.array(outputs2.logits.to('cpu'))))\n",
    "print(labels)\n",
    "print(out1)\n",
    "print(out2)\n",
    "\n",
    "answer = []\n",
    "for out1, out2 in zip(output1,output2):\n",
    "#     print(out1,out2)\n",
    "    if out1[0] == out2[0]:\n",
    "#         print(f'answer : {out1[1]}')\n",
    "        answer.append(out1[0])\n",
    "        \n",
    "    elif out1[1]<4.6:\n",
    "        answer.append(out2[0])\n",
    "        \n",
    "    else:\n",
    "        answer.append(out1[0])\n",
    "answer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "outputs.logits + outputs2.logits\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# cf = confusion_matrix(preds.to('cpu'),val_batch['labels'])\n",
    "cr = classification_report(preds.to('cpu'),val_batch['labels'])\n",
    "print(cr)\n",
    "# cf\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# makedata\n",
    "state_dict_dir1 = os.path.join('/opt/ml','Pstage/0421/results_xlm-roberta-large11-40/009_accuracy_77.78%.ckpt')\n",
    "state_dict_dir2 = os.path.join('/opt/ml','Pstage/0421/1~results_xlm-roberta-large09-36/004_accuracy_84.90%.ckpt')\n",
    "\n",
    "test_data = load_data(\"/opt/ml/input/data/test/test.tsv\")\n",
    "test_label = test_data['label'].values\n",
    "\n",
    "tokenized_test = tokenized_dataset(test_data, tokenizer)\n",
    "\n",
    "# config = AutoConfig.from_pretrained(model_name, num_labels = num_classes)\n",
    "# test_model = BertForSequenceClassification.from_pretrained(model_name, config= config)\n",
    "\n",
    "config2 = AutoConfig.from_pretrained(model_name, num_labels = num_classes)\n",
    "test_model1 = AutoModelForSequenceClassification.from_pretrained(model_name, config = config2)\n",
    "\n",
    "test_model1.load_state_dict(torch.load(state_dict_dir1))\n",
    "test_model1.to(device)\n",
    "\n",
    "\n",
    "config2 = AutoConfig.from_pretrained(model_name, num_labels = num_classes)\n",
    "test_model2 = AutoModelForSequenceClassification.from_pretrained(model_name, config = config2)\n",
    "\n",
    "test_model2.load_state_dict(torch.load(state_dict_dir2))\n",
    "test_model2.to(device)\n",
    "\n",
    "\n",
    "# test_dataset = RE_Dataset(tokenized_test, test_label)\n",
    "\n",
    "# test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "test_model1.eval()\n",
    "test_model2.eval()\n",
    "output1, output2 = [], []\n",
    "\n",
    "answer = []\n",
    "\n",
    "for data in tqdm(val_loader):\n",
    "    with torch.no_grad():\n",
    "        inputs = {key : value.to(device) for key, value in data.items() if key != 'labels'}\n",
    "        pred1 = test_model1(**inputs)\n",
    "        pred2 = test_model2(**inputs)\n",
    "        \n",
    "#         out1 = list(map(lambda x : (np.argmax(x), np.max(x)), np.array(pred1.logits.to('cpu'))))\n",
    "#         out2 = list(map(lambda x : (np.argmax(x), np.max(x)), np.array(pred2.logits.to('cpu'))))\n",
    "        \n",
    "# #         result = np.argmax(pred.logits.detach().cpu().numpy(), axis = -1)\n",
    "# #         print(result)\n",
    "#         output1.extend(out1)\n",
    "#         output2.extend(out2)\n",
    "        \n",
    "#         out1 = list(map(lambda x : (np.argmax(x), np.max(x)), np.array(pred1.logits.to('cpu'))))\n",
    "#         out2 = list(map(lambda x : (np.argmax(x), np.max(x)), np.array(pred2.logits.to('cpu'))))\n",
    "#         print(labels)\n",
    "#         print(out1)\n",
    "#         print(out2)\n",
    "        output1.extend(pred1.logits.to('cpu'))\n",
    "        output2.extend(pred2.logits.to('cpu'))\n",
    "        \n",
    "\n",
    "from IPython.display import Audio\n",
    "Audio(\"https://upload.wikimedia.org/wikipedia/commons/0/05/Beep-09.ogg\", autoplay=True)\n",
    "\n",
    "print(len(output1))\n",
    "print(len(output2))\n",
    "\n",
    "print(len(val_label))\n",
    "\n",
    "# output1\n",
    "\n",
    "torch.cat((output1,output2),0)\n",
    "\n",
    "make_valset = []\n",
    "for zero_data, nonzero_data in zip(output1,output2):\n",
    "#     zero_lab, zero_val = zero_data\n",
    "#     nonzero_lab, nonzero_val = nonzero_data\n",
    "#     print(zero_lab, zero_val, nonzero_lab, nonzero_val, label)\n",
    "#     make_dataset.append([zero_lab, zero_val, nonzero_lab, nonzero_val])\n",
    "#     print(zero_data)\n",
    "#     print(nonzero_data)\n",
    "    make_valset.append(torch.cat((zero_data,nonzero_data), -1))\n",
    "    \n",
    "#     break\n",
    "make_valset\n",
    "\n",
    "len(make_trainset)\n",
    "len(make_trainset[-1])\n",
    "\n",
    "pd.DataFrame(make_trainset).to_csv('./make_trainset.csv', index = False)\n",
    "pd.DataFrame(train_label).to_csv('./train_label.csv', index = False)\n",
    "pd.DataFrame(make_valset).to_csv('./make_valset.csv', index = False)\n",
    "pd.DataFrame(val_label).to_csv('./val_label.csv', index = False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# inference\n",
    "state_dict_dir1 = os.path.join('/opt/ml','Pstage/0421/results_xlm-roberta-large11-40/009_accuracy_77.78%.ckpt')\n",
    "state_dict_dir2 = os.path.join('/opt/ml','Pstage/0421/1~results_xlm-roberta-large09-36/004_accuracy_84.90%.ckpt')\n",
    "\n",
    "test_data = load_data(\"/opt/ml/input/data/test/test.tsv\")\n",
    "test_label = test_data['label'].values\n",
    "\n",
    "tokenized_test = tokenized_dataset(test_data, tokenizer)\n",
    "\n",
    "# config = AutoConfig.from_pretrained(model_name, num_labels = num_classes)\n",
    "# test_model = BertForSequenceClassification.from_pretrained(model_name, config= config)\n",
    "\n",
    "config2 = AutoConfig.from_pretrained(model_name, num_labels = num_classes)\n",
    "test_model1 = AutoModelForSequenceClassification.from_pretrained(model_name, config = config2)\n",
    "\n",
    "test_model1.load_state_dict(torch.load(state_dict_dir1))\n",
    "test_model1.to(device)\n",
    "\n",
    "\n",
    "config2 = AutoConfig.from_pretrained(model_name, num_labels = num_classes)\n",
    "test_model2 = AutoModelForSequenceClassification.from_pretrained(model_name, config = config2)\n",
    "\n",
    "test_model2.load_state_dict(torch.load(state_dict_dir2))\n",
    "test_model2.to(device)\n",
    "\n",
    "\n",
    "test_dataset = RE_Dataset(tokenized_test, test_label)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "test_model1.eval()\n",
    "test_model2.eval()\n",
    "output1, output2 = [], []\n",
    "\n",
    "answer = []\n",
    "\n",
    "for data in tqdm(test_loader):\n",
    "    with torch.no_grad():\n",
    "        inputs = {key : value.to(device) for key, value in data.items() if key != 'labels'}\n",
    "        pred1 = test_model1(**inputs)\n",
    "        pred2 = test_model2(**inputs)\n",
    "        \n",
    "#         out1 = list(map(lambda x : (np.argmax(x), np.max(x)), np.array(pred1.logits.to('cpu'))))\n",
    "#         out2 = list(map(lambda x : (np.argmax(x), np.max(x)), np.array(pred2.logits.to('cpu'))))\n",
    "        \n",
    "# #         result = np.argmax(pred.logits.detach().cpu().numpy(), axis = -1)\n",
    "# #         print(result)\n",
    "#         output1.extend(out1)\n",
    "#         output2.extend(out2)\n",
    "        \n",
    "        out1 = list(map(lambda x : (np.argmax(x), np.max(x)), np.array(pred1.logits.to('cpu'))))\n",
    "        out2 = list(map(lambda x : (np.argmax(x), np.max(x)), np.array(pred2.logits.to('cpu'))))\n",
    "#         print(labels)\n",
    "#         print(out1)\n",
    "#         print(out2)\n",
    "        output1.extend(out1)\n",
    "        output2.extend(out2)\n",
    "        \n",
    "\n",
    "for out1, out2 in zip(output1,output2):\n",
    "#     print(out1,out2)\n",
    "    if out1[0] == out2[0]:\n",
    "#         print(f'answer : {out1[1]}')\n",
    "        answer.append(out1[0])\n",
    "\n",
    "    elif out1[1]<4.6:\n",
    "        answer.append(out2[0])\n",
    "\n",
    "    else:\n",
    "        answer.append(out1[0])\n",
    "\n",
    "# print(output1)\n",
    "output = pd.DataFrame(np.array(answer).flatten(), columns=['pred'])\n",
    "output.to_csv('./submission_{}.csv'.format(time.strftime('%H-%M', time.localtime(time.time()))), index = False)\n",
    "\n",
    "answer = []\n",
    "for out1, out2 in zip(output1,output2):\n",
    "    print(out1,out2)\n",
    "    if out1[1] == out2[1]:\n",
    "        print(f'answer : {out1[1]}')\n",
    "        answer.append(out1[1])\n",
    "        \n",
    "    elif out1[0]<out2[0]:\n",
    "        answer.append(out2[1])\n",
    "        \n",
    "    break\n",
    "\n",
    "np.array(output_pred).flatten()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
